# ğŸ”’ Smart Audio Firewall

## ğŸ“š Table of Contents

* [Overview](#overview)
* [Features](#features)
* [Architecture](#architecture)
* [Implementation Details](#implementation-details)
* [Repository Structure](#repository-structure)
* [Prerequisites](#prerequisites)
* [How to Get Started](#how-to-get-started)
* [Logging](#logging)
* [Limitations](#limitations)
* [Future Enhancements](#future-enhancements)
* [Results](#results)
* [License](#license)

---

## ğŸ§½ Overview

Smart Audio Firewall is an AI-driven system that listens to ambient conversations â€” in physical rooms, video calls, or recorded meetings â€” and **intelligently flags, redacts ** content based on user-defined sensitive topics or trigger phrases.

It acts as an **AI content compliance filter**, alerting users when private or inappropriate speech is detected â€” even if indirectly phrased.

---
## âœ¨ Features

* ğŸ¹ **Multi-source Audio Input** â€“ Mic, MP3/WAV, MP4
* ğŸ§  **ASR via Whisper** â€“ Converts speech to structured text
* ğŸŒ **Language Detection** â€“ Automatically detects and flags non-English language use
* âš ï¸ **Sensitive Content Flagging**:
  * `Safe`: Acceptable content
  * `Warning`: Possibly sensitive, rephrased
  * `Critical`: Redacted
* âœ‚ï¸ **Redacted Output** â€“ Critical content is removed
* ğŸ§¾ **Reason Logging** â€“ Flags include rationale for decisions
* ğŸ“‹ **Transcript Output** â€“ Full text output (raw and redacted)
* ğŸ“„ **Reports** â€“ Generates  JSON, and TXT summaries
* ğŸ–¥ï¸ **Multiple Interfaces** â€“ CLI, Tkinter GUI, and Streamlit Web UI(In Progress)

---

## ğŸ›¡ï¸ Architecture

```
Audio Input (Mic/File)
     â†“
Audio Recorder / Loader
     â†“
Whisper Transcriber
     â†“
Ollama + Mistral (LLM)
     â†“
Segment Classification
     â†“
Redaction / Rephrasing
     â†“
 TXT / JSON Outputs
```

> ğŸ“Œ All outputs are logged and timestamped. Each segment includes confidence score, flag, and reasoning.

---
## ğŸ“˜ Full Architecture & Design Rationale

Want to understand how this system was built, why I made key decisions, and how I moved from embeddings to LLM-based classification?

ğŸ”— [Read the detailed Notion write-up](https://ruby-quotation-9e7.notion.site/Smart-Audio-Firewall-Architecture-Rationale-1fc2c68ffe0780c3a5bbcc581deb61b1)

## ğŸ›  Implementation Details

* **Whisper**: Converts audio to timestamped segments
* **Ollama + Mistral LLM**: Classifies and rewrites flagged text
* **FFmpeg**: Converts formats (e.g., MP4 to MP3)
* **PDF Report**: Built via `fpdf`
* **Logs**: Written to console and file
* **Outputs**: Structured as text, JSON outputs

---

## ğŸ“ Repository Structure
```
ProjectChallenge1/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ audio_input/
â”‚   â”‚   â”œâ”€â”€ Audio_Recording.py      # Mic or file input
â”‚   â”‚   â””â”€â”€ Transcriber.py          # Whisper transcription
â”‚   â””â”€â”€ text_input/
â”‚   â”‚   â””â”€â”€ llm_handler.py          # LLM-based classification and redaction
â”‚   â”œâ”€â”€ main.py                     # Streamlit implementation
â”œâ”€â”€ audio_data/
â”‚   â””â”€â”€ audio_files/                # Uploaded and recorded audio
â”œâ”€â”€ logs/                           # Application logs
â”œâ”€â”€ Outputs/                        # Redacted results, reports
â”œâ”€â”€ temp_uploads/                   # Intermediate storage
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ helpers.py                  # PDF & format utilities
â”‚   â”œâ”€â”€ json_io.py                  # JSON load/save
â”‚   â”œâ”€â”€ logger.py                   # Logging setup
â”‚   â””â”€â”€ paths.py                    # Folder and path management
â”‚
â”œâ”€â”€ run.py                          # CLI interface
â”œâ”€â”€ start_app.py                    # Streamlit entry
â”œâ”€â”€ start_gui.py                    # Tkinter desktop GUI

```
---

## âš™ï¸ Prerequisites

* Python 3.10+
* [FFmpeg](https://ffmpeg.org/download.html)
* [Ollama](https://ollama.com/download)
* `pyaudio` for microphone input

---

## ğŸš€ How to Get Started

### Note : Create a Virtual Environment to avoid dependency errors

```bash
python -m venv venv 
.venv\Scripts\activate
```
### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Start Ollama and load a model

```bash
ollama serve
ollama run mistral
```

### 3. Run the app

#### CLI

```bash
python run.py --use-file ./example.mp3 --topics salary gossip harassment 
python run.py --> To record audio
```

#### Tkinter GUI

```bash
python start_gui.py
```

#### Streamlit Web UI (In Progress: Not Recommended as of now because of the web timed out errors due to latency issues)

```bash
python start_app.py
```

---

## ğŸ“ Logging

All actions and errors are logged to:

```
logs/sessions.txt
```

Includes:

* Audio file events
* Transcription progress
* LLM call results
* Redaction actions and rationales

---

## âš ï¸ Limitations

Despite strong functionality, the system has some architectural constraints:

1. **LLM Dependency**
   Results rely heavily on the accuracy and consistency of LLM completions.

2. **Security Concerns**
   Text is sent to a local LLM request. This may not meet enterprise-grade data protection needs without isolation.

3. **Untested Edge Cases**
   No formal unit or integration test suite is included yet. Failures may go unnoticed during upgrades or refactors.

4. **Performance Bottlenecks**
   Running LLMs like Mistral locally on CPU leading to slow response times, impacting real-time usability.

---

## ğŸ”® Future Enhancements

* ğŸŒ Multilingual Support -automatic translation + classification
* ğŸ”„ Plugin Integration â€“  Zoom, Slack, MS Teams integration
* ğŸ“€ Backend Database â€“ User history, configurations, audit logs
* ğŸ‘¥ Session Management â€“ reprocessing, version history
* ğŸ§  LLM/ Transformer models Fine-Tuning â€“ Custom models for domain-specific language and use cases

---

## âœ… Results

All outputs are stored in the `Outputs/` directory:

* `classified_transcript_*.json` â€“ Full segment + reasoning
![img_2.png](images/img_2.png)
* `redacted_text_*.txt` â€“ Sanitized speech output
  ![img_1.png](images/img_1.png)
* `privacy_report_*.txt` â€“ Segment summary
  ![img.png](images/img.png)

---

## ğŸ“„ License

MIT â€” free to use, modify, and distribute.

---
